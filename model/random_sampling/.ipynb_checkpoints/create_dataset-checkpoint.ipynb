{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATED BY MATIAS BENITEZ 08/03/2018\n",
    "#code for creating CT_L_clean, CT_L2_clean and CT_U_clean, with fiels FInd&Impr and also CAP\n",
    "#it creates the vocabulary and inputs for the model by excluding text without findings and impressions and exluding the sections before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import files\n",
    "CT_L = pd.read_excel('M0.xlsx')\n",
    "CT_U = pd.read_csv('CT_U.csv')\n",
    "CT_L2 = pd.read_excel('table.xlsm')\n",
    "test=pd.read_csv('Test_set.csv')\n",
    "#CT_L = CT_L[:230]\n",
    "#Match column names\n",
    "CT_L2 = CT_L2.rename(columns = {'Other' : 'Foreign Body'})\n",
    "CT_U = CT_U[:26383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_L2=CT_L2.loc[(CT_L2['findings+impress']==1)&(CT_L2.duplicate==0) & (CT_L2.exam_group=='CAP'), ['Report#', 'NOTE_TEXT_1+2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Report#</th>\n",
       "      <th>FakeID</th>\n",
       "      <th>study_flag</th>\n",
       "      <th>MRN</th>\n",
       "      <th>ACCESSION_NUM</th>\n",
       "      <th>DEPARTMENT_NAME</th>\n",
       "      <th>PAT_ENC_CSN_ID</th>\n",
       "      <th>NOTE_DTTM</th>\n",
       "      <th>exam_group</th>\n",
       "      <th>NOTE_PROC_DESC</th>\n",
       "      <th>findings+impress</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>NOTE_TEXT_1+2</th>\n",
       "      <th>Lungs</th>\n",
       "      <th>Liver + gall bladder</th>\n",
       "      <th>Kidneys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>CTAAEC</td>\n",
       "      <td>Set A</td>\n",
       "      <td>T51634</td>\n",
       "      <td>AA3736356</td>\n",
       "      <td>DUH EMERGENCY DEPT</td>\n",
       "      <td>155464004.0</td>\n",
       "      <td>2017-01-01 01:24:00</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CT chest abdomen pelvis with contrast w MIPS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>CTAAEH</td>\n",
       "      <td>Set A</td>\n",
       "      <td>DA4483</td>\n",
       "      <td>AA3736450</td>\n",
       "      <td>DMP 6W TRAUMA SICU</td>\n",
       "      <td>155211501.0</td>\n",
       "      <td>2017-01-01 04:29:00</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CT chest abdomen pelvis with contrast w MIPS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>CTAAEY</td>\n",
       "      <td>Set A</td>\n",
       "      <td>D2010756</td>\n",
       "      <td>AA3736648</td>\n",
       "      <td>DUH N3100 CARDIOTHORACIC SURGERY</td>\n",
       "      <td>153627628.0</td>\n",
       "      <td>2017-01-01 12:20:00</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CT chest abdomen pelvis without contrast with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CT Chest without IV Contrast   CT Abdomen and ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>CTAAFE</td>\n",
       "      <td>Set A</td>\n",
       "      <td>CK7037</td>\n",
       "      <td>AA3736903</td>\n",
       "      <td>DRH 41 GENERAL MEDICINE</td>\n",
       "      <td>155456952.0</td>\n",
       "      <td>2017-01-01 13:23:00</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CT chest with contrast</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EXAM:  CT Chest with contrast    INDICATION: S...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>CTAAFS</td>\n",
       "      <td>Set A</td>\n",
       "      <td>D2049058</td>\n",
       "      <td>AA3736367</td>\n",
       "      <td>DMP 8W NEUROLOGY/NEUROSURGERY</td>\n",
       "      <td>155441695.0</td>\n",
       "      <td>2017-01-01 16:09:00</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CT chest abdomen pelvis with contrast w MIPS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Report#  FakeID study_flag       MRN ACCESSION_NUM  \\\n",
       "0           5        6  CTAAEC      Set A    T51634     AA3736356   \n",
       "1          10       11  CTAAEH      Set A    DA4483     AA3736450   \n",
       "2          27       28  CTAAEY      Set A  D2010756     AA3736648   \n",
       "3          33       34  CTAAFE      Set A    CK7037     AA3736903   \n",
       "4          47       48  CTAAFS      Set A  D2049058     AA3736367   \n",
       "\n",
       "                    DEPARTMENT_NAME  PAT_ENC_CSN_ID            NOTE_DTTM  \\\n",
       "0                DUH EMERGENCY DEPT     155464004.0  2017-01-01 01:24:00   \n",
       "1                DMP 6W TRAUMA SICU     155211501.0  2017-01-01 04:29:00   \n",
       "2  DUH N3100 CARDIOTHORACIC SURGERY     153627628.0  2017-01-01 12:20:00   \n",
       "3           DRH 41 GENERAL MEDICINE     155456952.0  2017-01-01 13:23:00   \n",
       "4     DMP 8W NEUROLOGY/NEUROSURGERY     155441695.0  2017-01-01 16:09:00   \n",
       "\n",
       "  exam_group                                     NOTE_PROC_DESC  \\\n",
       "0        CAP       CT chest abdomen pelvis with contrast w MIPS   \n",
       "1        CAP       CT chest abdomen pelvis with contrast w MIPS   \n",
       "2        CAP  CT chest abdomen pelvis without contrast with ...   \n",
       "3        CAP                             CT chest with contrast   \n",
       "4        CAP       CT chest abdomen pelvis with contrast w MIPS   \n",
       "\n",
       "   findings+impress  duplicate  \\\n",
       "0                 1          0   \n",
       "1                 1          0   \n",
       "2                 1          0   \n",
       "3                 1          0   \n",
       "4                 1          0   \n",
       "\n",
       "                                       NOTE_TEXT_1+2  Lungs  \\\n",
       "0  CT Chest with IV Contrast   CT Abdomen and Pel...    1.0   \n",
       "1  CT Chest with IV Contrast   CT Abdomen and Pel...    1.0   \n",
       "2  CT Chest without IV Contrast   CT Abdomen and ...    0.0   \n",
       "3  EXAM:  CT Chest with contrast    INDICATION: S...    1.0   \n",
       "4  CT Chest with IV Contrast   CT Abdomen and Pel...    0.0   \n",
       "\n",
       "   Liver + gall bladder  Kidneys  \n",
       "0                   0.0      1.0  \n",
       "1                   1.0      1.0  \n",
       "2                   0.0      0.0  \n",
       "3                   1.0      0.0  \n",
       "4                   0.0      0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=test\n",
    "print(len(p))\n",
    "p.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import excel files containing reviews\n",
    "review1=pd.read_excel('M1.xlsx')\n",
    "review2=pd.read_excel('M2.xlsx')\n",
    "review3=pd.read_excel('M3.xlsx')\n",
    "#val=pd.read_csv('validation_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill the labels having A's and NaN with 0\n",
    "#CT_L['NOTE_TEXT_2'].replace(np.nan, ' ', inplace=True)\n",
    "#CT_L['NOTE_TEXT_1'].replace(np.nan, ' ', inplace=True)\n",
    "#CT_L.replace('A', 0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [u'Lung', u'Liver+GallBladder',u'Kidney']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>NOTE_PROC_DESC</th>\n",
       "      <th>NOTE_TEXT_1+2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7283.0</td>\n",
       "      <td>PET CT chest with contrast</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7284.0</td>\n",
       "      <td>PET CT abdomen and pelvis with contrast</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9387.0</td>\n",
       "      <td>CT chest PE protocol incl CT angiogram chest w...</td>\n",
       "      <td>Chest CTA    Technique:  Chest CTA PE Protocol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9388.0</td>\n",
       "      <td>CT abdomen pelvis with contrast</td>\n",
       "      <td>CT abdomen and pelvis with IV contrast    Comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24255.0</td>\n",
       "      <td>CT chest abdomen pelvis with contrast w MIPS</td>\n",
       "      <td>CT Chest with IV Contrast   CT Abdomen and Pel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject                                     NOTE_PROC_DESC  \\\n",
       "0   7283.0                         PET CT chest with contrast   \n",
       "1   7284.0            PET CT abdomen and pelvis with contrast   \n",
       "2   9387.0  CT chest PE protocol incl CT angiogram chest w...   \n",
       "3   9388.0                    CT abdomen pelvis with contrast   \n",
       "4  24255.0       CT chest abdomen pelvis with contrast w MIPS   \n",
       "\n",
       "                                       NOTE_TEXT_1+2  \n",
       "0  CT Chest with IV Contrast   CT Abdomen and Pel...  \n",
       "1  CT Chest with IV Contrast   CT Abdomen and Pel...  \n",
       "2  Chest CTA    Technique:  Chest CTA PE Protocol...  \n",
       "3  CT abdomen and pelvis with IV contrast    Comp...  \n",
       "4  CT Chest with IV Contrast   CT Abdomen and Pel...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_U.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review2=review2.join(CT_U[['NOTE_TEXT_1+2', 'Subject']].set_index('Subject'), on='Report#')\n",
    "review2.drop('Unnamed: 4', axis=1, inplace=True)\n",
    "review2.dropna(inplace=True)\n",
    "review3=review3.join(CT_U[['NOTE_TEXT_1+2', 'Subject']].set_index('Subject'), on='Report#')\n",
    "review3.drop('Unnamed: 4', axis=1, inplace=True)\n",
    "review3.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'Lungs':'Lung','Kidneys':'Kidney','Liver + gall bladder':'Liver+GallBladder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.loc[:,['Report#', 'Lung', 'Liver+GallBladder', 'Kidney', 'NOTE_TEXT_1+2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns true if a note has findings and impressions\n",
    "def bool_findings(note):\n",
    "    c=re.compile(r'(?i)findings:')\n",
    "    d=re.compile(r'(?i)impression:')\n",
    "    return  bool(c.search(note)) and bool(d.search(note))\n",
    "#get rid of text before findings\n",
    "def extract_findings(note):\n",
    "    c=re.compile(r'(?i)findings:')\n",
    "    d=re.compile(r'(?i)impression:')\n",
    "    start=0\n",
    "    end=re.compile(r'(?i)electronically')\n",
    "    end_=-1\n",
    "    if bool(c.search(note)):\n",
    "        start=re.search(c, note).start()\n",
    "        if bool(end.search(note)):\n",
    "            end_=re.search(end, note).start()\n",
    "        return note[start:end_]\n",
    "    #if bool(d.search(note)):\n",
    "    #    start=re.search(d, note).start()\n",
    "    #    try:\n",
    "    #        end_=re.search(end, note).start()\n",
    "    #    except:\n",
    "    #        pass\n",
    "    #    return note[start:end_]\n",
    "    #else:\n",
    "    #    return note\n",
    "#returns true if it belongs to CHest, abdomen and pelvis\n",
    "def bool_regions(a):\n",
    "    note=a.split('.')[0]\n",
    "    c=re.compile(r'(?i)abd')\n",
    "    d=re.compile(r'(?i)chest')\n",
    "    e=re.compile(r'(?i)pelv')\n",
    "    return  bool(c.search(note)) and bool(d.search(note)) and bool(e.search(note))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine columns containing texts\n",
    "#CT_L['NOTE_TEXT_1+2']=(CT_L['NOTE_TEXT_1'] + CT_L['NOTE_TEXT_2']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns with true if it has findings and impression\n",
    "CT_L['Find&Impr']=CT_L['NOTE_TEXT_1+2'].apply(bool_findings)\n",
    "#CT_L2['Find&Impr']=CT_L2['NOTE_TEXT_1+2'].apply(bool_findings)\n",
    "review1['Find&Impr']=review1['NOTE_TEXT_1+2'].apply(bool_findings)\n",
    "review2['Find&Impr']=review2['NOTE_TEXT_1+2'].apply(bool_findings)\n",
    "review3['Find&Impr']=review3['NOTE_TEXT_1+2'].apply(bool_findings)\n",
    "CT_U['Find&Impr']=CT_U['NOTE_TEXT_1+2'].apply(bool_findings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns with true if it belongs to chest, abdomen and pelvis\n",
    "CT_L['CAP']=CT_L['NOTE_TEXT_1+2'].apply(bool_regions)\n",
    "#CT_L2['CAP']=CT_L2['NOTE_TEXT_1+2'].apply(bool_regions)\n",
    "#review1['CAP']=review1['note'].apply(bool_regions)\n",
    "#review2['CAP']=review2['note'].apply(bool_regions)\n",
    "#review3['CAP']=review3['note'].apply(bool_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_L.drop(['Key Words Missing','Questionable line'], axis=1, inplace=True)\n",
    "CT_L.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1.drop('Unnamed: 5', axis=1, inplace=True)\n",
    "review1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save files\n",
    "CT_L.to_csv('M0_clean.csv', index=False)\n",
    "CT_L2.to_csv('all_reports_clean.csv', index=False)\n",
    "review1.to_csv('M1_clean.csv', index=False)\n",
    "review2.to_csv('M2_clean.csv', index=False)\n",
    "review3.to_csv('M3_clean.csv', index=False)\n",
    "test.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of notes with findings and impressions\n",
    "text_r1=review1.loc[:,'NOTE_TEXT_1+2'].tolist()\n",
    "text_r2=review2.loc[:,'NOTE_TEXT_1+2'].tolist()\n",
    "text_r3=review3.loc[:,'NOTE_TEXT_1+2'].tolist()\n",
    "text_L = (CT_L.loc[:,'NOTE_TEXT_1+2']).tolist()\n",
    "text_U = (CT_U.loc[:,'NOTE_TEXT_1+2']).tolist()\n",
    "text_L2 = (CT_L2.loc[:,'NOTE_TEXT_1+2']).tolist()\n",
    "text_test=(test.loc[:,'NOTE_TEXT_1+2']).tolist()\n",
    "#text_val=(val.loc[:,'NOTE_TEXT_1+2']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_replace=text_L2[2366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of text before findings\n",
    "text_r1=[extract_findings(x) for x in text_r1]\n",
    "text_r2=[extract_findings(x) for x in text_r2]\n",
    "text_r3=[extract_findings(x) for x in text_r3]\n",
    "text_L=[extract_findings(x) for x in text_L]\n",
    "text_L2=[extract_findings(x) for x in text_L2]\n",
    "text_U=[extract_findings(x) for x in text_U]\n",
    "text_test=[extract_findings(x) for x in text_test]\n",
    "#text_val=[extract_findings(x) for x in text_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\numpy\\core\\fromnumeric.py:51: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2366"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_nan(list_):\n",
    "    return np.argmax(pd.Series(list_).isna()*1.0)\n",
    "list_nan(text_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_replace=str_replace[str_replace.index('Chest: '):str_replace.index('Electronically ')]\n",
    "text_L2[2366]=str_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chest:    Normal Thyroid.  No abnormality of the chest wall.    Heart is borderline enlarged. No pericardial effusion. Coronary  atherosclerosis.  No evidence of central pulmonary embolism.  No pulmonary  artery dilatation.      No enlarged supraclavicular, axillary or mediastinal lymph nodes.    Lungs are clear except for dependent atelectasis.   No pleural effusion.      Abdomen/Pelvis:    Liver contour is normal. No focal liver lesion. Portal and hepatic veins  are patent.  Normal gallbladder. No biliary dilatation. Pancreas is normal.  No mass or  peripancreatic inflammation. Normal spleen. Normal adrenal glands.    Mild prominence of the right renal pelvis. The left kidney is atrophic.    Stomach and small bowel are nondilated and noninflamed. Right lower  quadrant ostomy is noted. Small bowel and colon are noninflamed and  nondilated. Surgical clips in the right lower quadrant along the right  pelvic sidewall..    Bladder is surgically absent. Ileal conduit is noted. Prostate is  surgically absent.    No free air. No free fluid.      No enlarged retroperitoneal or mesenteric lymph nodes. There is mural  atherosclerosis of the aortoiliac vessels without aneurysm or significant  stenosis.    No acute fracture.  No aggressive bone lesions.       Impression:    1. No evidence for metastatic disease in the abdomen or pelvis.    '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_L2[2366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_asteric(serie):\n",
    "    return serie.apply(lambda x: 0 if x=='0*' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#MATRIX WITH LABELS\n",
    "label_L = CT_L.loc[:, label_list].fillna(0).apply(pd.to_numeric, ).astype(int).as_matrix()\n",
    "#label_L2 = CT_L2.loc[CT_L2['Find&Impr'], label_list].fillna(0).apply(pd.to_numeric, ).astype(int).as_matrix()\n",
    "label_r1= review1.loc[:, label_list].fillna(0).apply(pd.to_numeric, ).astype(int).as_matrix()\n",
    "label_r2= review2.loc[:, label_list].fillna(0).apply(pd.to_numeric, ).astype(int).as_matrix()\n",
    "label_r3= review3.loc[:, label_list].fillna(0).apply(pd.to_numeric, ).astype(int).as_matrix()\n",
    "label_test= test.loc[:, label_list].fillna(0).apply(del_asteric).apply(pd.to_numeric, ).astype(int).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "#     string = re.sub(r\"[^A-Za-z0-9(),\\.!?]\", \" \", string)  \n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)  # remove numbers\n",
    "    #string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"e\\.g\\.,\", \" \", string) \n",
    "    string = re.sub(r\"a\\.k\\.a\\.\", \" \", string) \n",
    "    string = re.sub(r\"i\\.e\\.,\", \" \", string) \n",
    "#     string = re.sub(r\"i\\.e\\.\", \" \", string) \n",
    "    #string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    #string = re.sub(r\"\\'\", \"\", string) \n",
    "    #string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    #string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    #string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"br\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\.\", \" . \", string)  \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string) \n",
    "    string = re.sub(r\"u\\.s\\.\", \" us \", string)\n",
    "    return string.strip().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize texts\n",
    "text_L_clean = [clean_str(l) for l in text_L]\n",
    "text_L2_clean = [clean_str(l) for l in text_L2]\n",
    "text_U_clean = [clean_str(l) for l in pd.Series(text_U).dropna().tolist()]\n",
    "text_r1_clean= [clean_str(l) for l in text_r1]\n",
    "text_r2_clean= [clean_str(l) for l in text_r2]\n",
    "text_r3_clean= [clean_str(l) for l in text_r3]\n",
    "text_test_clean=[clean_str(l) for l in text_test]\n",
    "#text_val_clean=[clean_str(l) for l in text_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list to construct vocabulary\n",
    "clean_text = text_U_clean\n",
    "clean_text_all = clean_text #+ text_L2_clean\n",
    "import itertools\n",
    "from collections import Counter\n",
    "word_clean_count = Counter(list(itertools.chain.from_iterable(clean_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict of unique words\n",
    "v_clean = [x for x, y in word_clean_count.items() if y >=5]\n",
    "v_clean_all = [x for x, y in word_clean_count.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680\n",
      "11104\n"
     ]
    }
   ],
   "source": [
    "print(len(v_clean))\n",
    "print(len(v_clean_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocabulary and inverse vocabulary. Words with frequency >5\n",
    "inv_vocabulary_clean ={}\n",
    "inv_vocabulary_clean[0] = 'END'\n",
    "inv_vocabulary_clean[1] = 'UNK'\n",
    "inv_vocabulary_clean[2] = '<PAD/>'\n",
    "vocabulary_clean ={}\n",
    "vocabulary_clean['END'] = 0\n",
    "vocabulary_clean['UNK'] = 1\n",
    "vocabulary_clean['<PAD/>'] = 2\n",
    "ix=3\n",
    "for v in v_clean_all:\n",
    "    if v in v_clean:\n",
    "        vocabulary_clean[v] = ix\n",
    "        inv_vocabulary_clean[ix] = v\n",
    "        ix +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert list of words to list of id's\n",
    "def convert_word_to_ix(data):\n",
    "    result = []\n",
    "    for sent in data:\n",
    "        temp = []\n",
    "        for w in sent:\n",
    "            if w in vocabulary:\n",
    "                temp.append(vocabulary.get(w,1))\n",
    "            else:\n",
    "                temp.append(1)\n",
    "        temp.append(0)\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert list of words to list of id's\n",
    "def convert_word_to_ix_clean(data):\n",
    "    result = []\n",
    "    for sent in data:\n",
    "        temp = []\n",
    "        for w in sent:\n",
    "            if w in vocabulary_clean:\n",
    "                temp.append(vocabulary_clean.get(w,1))\n",
    "            else:\n",
    "                temp.append(1)\n",
    "        temp.append(0)\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine initial labeled data\n",
    "train_x_clean = text_L_clean #+ text_L2_clean\n",
    "#train_y = np.append(label_L, label_L2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_x_clean = text_U_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of words to list of id's\n",
    "train_x_clean_num = convert_word_to_ix_clean(train_x_clean)\n",
    "text_L2_clean_num = convert_word_to_ix_clean(text_L2_clean)\n",
    "unlabeled_x_clean_num = convert_word_to_ix_clean(unlabeled_x_clean)\n",
    "r1_clean_idx= convert_word_to_ix_clean(text_r1_clean)\n",
    "r2_clean_idx= convert_word_to_ix_clean(text_r2_clean)\n",
    "r3_clean_idx= convert_word_to_ix_clean(text_r3_clean)\n",
    "test_clean_idx= convert_word_to_ix_clean(text_test_clean)\n",
    "#val_clean_idx= convert_word_to_ix_clean(text_val_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_x=max([len(s) for s in train_x_clean_num ])\n",
    "max_len_l2=max([len(s) for s in text_L2_clean_num ])\n",
    "max_len_unlab=max([len(s) for s in unlabeled_x_clean_num ])\n",
    "max_len_r1=max([len(s) for s in r1_clean_idx ])\n",
    "max_len_r2=max([len(s) for s in r2_clean_idx ])\n",
    "max_len_r3=max([len(s) for s in r3_clean_idx ])\n",
    "max_len_test=max([len(s) for s in test_clean_idx ])\n",
    "#max_len_val=max([len(s) for s in val_clean_idx ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#save initial labeled data to pickle file\n",
    "import pickle\n",
    "with open('M0.p', 'wb') as f:\n",
    "    pickle.dump([train_x_clean_num, train_x_clean, label_L, vocabulary_clean, inv_vocabulary_clean, max_len_x], f, protocol=2)\n",
    "    \n",
    "#save review1 to pickle file\n",
    "with open('M1.p', 'wb') as f:\n",
    "    pickle.dump([r1_clean_idx, text_r1_clean, label_r1, max_len_r1], f, protocol=2)\n",
    "\n",
    "#save review2 to pickle file\n",
    "import pickle\n",
    "with open('M2.p', 'wb') as f:\n",
    "    pickle.dump([r2_clean_idx, text_r2_clean, label_r2 , max_len_r2], f, protocol=2)\n",
    "\n",
    "with open('M3.p', 'wb') as f:\n",
    "    pickle.dump([r3_clean_idx, text_r3_clean, label_r3, max_len_r3], f, protocol=2)\n",
    "\n",
    "with open('CT_unlabel_clean.p', 'wb') as f:\n",
    "    pickle.dump([unlabeled_x_clean_num, unlabeled_x_clean,max_len_unlab], f, protocol=2)\n",
    "    \n",
    "with open('test.p', 'wb') as f:\n",
    "    pickle.dump([test_clean_idx, text_test_clean, label_test, max_len_test], f, protocol=2)   \n",
    "with open('all_valid_reports.p', 'wb') as f:\n",
    "    pickle.dump([text_L2_clean_num, text_L2_clean, max_len_l2], f, protocol=2)  \n",
    "    \n",
    "import gensim\n",
    "model_clean = gensim.models.Word2Vec(unlabeled_x_clean, size=300, window=5, min_count=5, workers=4)\n",
    "model_clean.wv.save_word2vec_format('CTword2vec_clean','CTvocab_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN UNTIL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_clean = gensim.models.Word2Vec(unlabeled_x_clean, size=70, window=5, min_count=5, workers=4)\n",
    "model_clean.wv.save_word2vec_format('CTword2vec_clean_70','CTvocab_clean_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5581"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of vocabulary\n",
    "len(vocabulary_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CT_U[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maximum length of reports\n",
    "max(len(i) for i in unlabeled_x_clean_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to make all notes of the same length by adding PAD before and after the text.\n",
    "def pad_sentences_clean(sentences, padding_word=\"<PAD/>\", pre_pad_length=5,vocab=vocabulary_clean, inv_vocab=inv_vocabulary_clean, sequence_length=528):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "#     sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    num_sentences = []\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        sentence_word = [inv_vocab[i] for i in sentence]\n",
    "        new_sentence = [padding_word] * pre_pad_length + sentence_word + [padding_word] * num_padding \\\n",
    "                        + [padding_word] * pre_pad_length\n",
    "        padded_sentences.append(new_sentence)\n",
    "        num_sentences.append([vocab[i] for i in new_sentence])\n",
    "    return padded_sentences, num_sentences, sequence_length + 2 * pre_pad_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99, 538)\n"
     ]
    }
   ],
   "source": [
    "#PADD initial labeled data\n",
    "x_train_clean_padded, x_train_clean_num, sentence_length_clean = pad_sentences_clean(train_x_clean_num )\n",
    "print(len(x_train_clean_padded), len(x_train_clean_num), sentence_length_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99, 538)\n",
      "(99, 99, 538)\n",
      "(99, 99, 538)\n"
     ]
    }
   ],
   "source": [
    "#PADD reviews\n",
    "r1_clean_padded, r1_clean_num, sentence_length_clean = pad_sentences_clean(r1_clean_idx)\n",
    "print(len(r1_clean_padded), len(r1_clean_num), sentence_length_clean)\n",
    "r2_clean_padded, r2_clean_num, sentence_length_clean = pad_sentences_clean(r2_clean_idx)\n",
    "print(len(r2_clean_padded), len(r2_clean_num), sentence_length_clean)\n",
    "r3_clean_padded, r3_clean_num, sentence_length_clean = pad_sentences_clean(r3_clean_idx)\n",
    "print(len(r3_clean_padded), len(r3_clean_num), sentence_length_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 301, 538)\n"
     ]
    }
   ],
   "source": [
    "#PADD initial labeled data\n",
    "x_test_clean_padded, x_test_clean_num, sentence_length_clean = pad_sentences_clean(test_clean_idx )\n",
    "print(len(x_test_clean_padded), len(x_test_clean_num), sentence_length_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24084, 24084, 538)\n"
     ]
    }
   ],
   "source": [
    "#PADD unlabeled data\n",
    "x_unlabel_clean_padded, x_unlabel_clean_num, sentence_length_clean = pad_sentences_clean(unlabeled_x_clean_num )\n",
    "print(len(x_unlabel_clean_padded), len(x_unlabel_clean_num), sentence_length_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save initial labeled data to pickle file\n",
    "import pickle\n",
    "with open('M0.p', 'wb') as f:\n",
    "    pickle.dump([x_train_clean_num, x_train_clean_padded, label_L, vocabulary_clean, inv_vocabulary_clean], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save review1 to pickle file\n",
    "import pickle\n",
    "with open('M1.p', 'wb') as f:\n",
    "    pickle.dump([r1_clean_num, r1_clean_padded, label_r1], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save review2 to pickle file\n",
    "import pickle\n",
    "with open('M2.p', 'wb') as f:\n",
    "    pickle.dump([r2_clean_num, r2_clean_padded, label_r2], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save review2 to pickle file\n",
    "import pickle\n",
    "with open('M3.p', 'wb') as f:\n",
    "    pickle.dump([r3_clean_num, r3_clean_padded, label_r3], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save unlabeled data to pickle file\n",
    "import pickle\n",
    "with open('CT_unlabel_clean.p', 'wb') as f:\n",
    "    pickle.dump([x_unlabel_clean_num, x_unlabel_clean_padded], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save word2vec model\n",
    "import gensim\n",
    "model_clean = gensim.models.Word2Vec(clean_text, size=80, window=5, min_count=5, workers=4)\n",
    "model_clean.wv.save_word2vec_format('CTword2vec_clean','CTvocab_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save review2 to pickle file\n",
    "import pickle\n",
    "with open('test.p', 'wb') as f:\n",
    "    pickle.dump([x_test_clean_num, x_test_clean_padded, label_test], f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
